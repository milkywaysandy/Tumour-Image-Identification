# -*- coding: utf-8 -*-
"""cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRDeeReocfbvYIryQWaE7Viffp9T298u

##**Histopathologic Cancer Detection**

Challenge Problem:

This Kaggle competition is a binary image classification problem where you will identify metastatic cancer in small image patches taken from larger digital pathology scans.

Basic Data information (size, deminsion, structures):

*   size = 7.76Gb
*   32x32px region of a patch contains at least one pixel of tumor tissue

*Dataset:* There are 220,025 images with labels and without missing values or duplicates. This is a cleaned well-label dataset. The number matches between the .csv file and the numbers of images in the train/test folder.
"""

from google.colab import drive #link this file to the google drive (use it only when you want to import file from google drive )
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import glob
from tqdm import tqdm
from PIL import Image

!unzip /content/gdrive/MyDrive/Data_Science/Cancer/Data/histopathologic-cancer-detection.zip -d /content/cancer
!pip install tensorflow

# Define paths & load training dataset labels
data_dir = '/content/cancer'
train_images_dir = os.path.join(data_dir, 'train')
test_images_dir = os.path.join(data_dir, 'test')
label = pd.read_csv('/content/cancer/train_labels.csv')

"""##**Exploratory Data Analysis (EDA) â€” Inspect, Visualize and Clean the Data (15 pts)**

**Class distribution:**

There is a class imbalance with 120000 of non-cancerous(0) vs 80000 of cancerous(1). **I planed to balance the class by reduce both class to 50000 before training.**
***
**Sample image Visualization:**

A display of sample images for both non-cancerous and cancerous classes helps visualization of the data. This confirms the type of classification task needs to extract with deep learning network, such as CNN.
***
**Plan of Analysis**

The reasoning for reducing the dataset to 50000 samples per class is primarily to address the class imbalance identified during the Exploratory Data Analysis. The original dataset had significantly more non-cancerous samples than cancerous ones. By sampling an equal number from each class, the goal is to create a balanced dataset, which can help prevent the model from being biased towards the majority class during training and potentially improve its performance on the minority class.
"""

label.head()

# Check for duplicates
print("\nDuplicate rows in training labels:")
print(label[label.duplicated(keep=False)])

# Class distribution
class_counts = label['label'].value_counts()
print("\nClass distribution:", class_counts)

# found number of files in both test & train folder
def count_files_in_directory(directory_path):
    file_count = 0
    for entry in os.scandir(directory_path):
        if entry.is_file():
            file_count += 1
    return file_count

print(f"Number of files in {"train"}: {count_files_in_directory(train_images_dir)}")
print(f"Number of files in {"test"}: {count_files_in_directory(test_images_dir)}")

# Visualize class distribution
plt.figure(figsize=(4, 3))
sns.countplot(x='label', data=label)
plt.title('Class Distribution (0: Non-Cancerous, 1: Cancerous)')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Image statistics
sample_image = Image.open(os.path.join(train_images_dir, f'{label["id"].iloc[0]}.tif'))
print(f"\nSample image dimensions: {sample_image.size}")
print(f"Sample image mode: {sample_image.mode}")

# Display sample images
def display_sample_images(df, image_dir, num_samples=3):
    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples*4, 8))
    for i, label in enumerate([0, 1]):
        label_df = df[df['label'] == label].sample(num_samples, random_state=47)
        for j, row in enumerate(label_df.itertuples()):
            img_path = os.path.join(image_dir, f'{row.id}.tif')
            img = Image.open(img_path)
            axes[i, j].imshow(img)
            axes[i, j].set_title(f'Label: {label}')
            axes[i, j].axis('off')
    plt.suptitle('Sample Images (Top: Non-Cancerous, Bottom: Cancerous)')
    plt.show()

display_sample_images(label, train_images_dir)

"""## Data Cleaning & Structure Preparation

The data cleaning procedure involved checking for duplicates and addressing class imbalance. The dataset was described as already cleaned, without missing values or duplicates. The main cleaning step mentioned is handling the class imbalance by reducing the number of samples in each class to 50000 before training. This was done to create a balanced dataset for training the model.
"""

# TensorFlow
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, callbacks

# sklearn stuff
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Create balanced dataset with 50000 samples of each label
df_train_balanced_0 = label[label['label'] == 0].sample(50000, random_state=42)
df_train_balanced_1 = label[label['label'] == 1].sample(50000, random_state=42) # Sample 50000 from label 1 as well
df_train = pd.concat([df_train_balanced_0, df_train_balanced_1])

# Shuffle the balanced dataset
df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)

print("Class distribution in the newly balanced dataset:", df_train['label'].value_counts())

# Add 'filename' and 'label_str' columns to the new df_train
df_train['filename'] = df_train['id'] + '.tif'
df_train['label_str'] = df_train['label'].astype(str)

# size of the images & batch size
IMG_SIZE = (32, 32)
BATCH_SIZE = 64

def extract_center(img):
    # center coordinates
    h, w = img.shape[0], img.shape[1]
    center_h, center_w = h // 2, w // 2
    offset = 16  # 32/2 pixels

    # get center 32x32 region
    center_img = img[center_h-offset:center_h+offset, center_w-offset:center_w+offset, :]
    return center_img

datagen = ImageDataGenerator(
    rescale=1./255,
    preprocessing_function=extract_center,
    validation_split=0.2)

train_generator = datagen.flow_from_dataframe(
    dataframe=df_train,
    directory=train_images_dir,
    x_col='filename',
    y_col='label_str',
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='training',
    shuffle=True)
print(f"Training generator batches: {len(train_generator)}")

validation_generator = datagen.flow_from_dataframe(
    dataframe=df_train,
    directory=train_images_dir,
    x_col='filename',
    y_col='label_str',
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='validation',
    shuffle=False)
print(f"Validation generator batches: {len(validation_generator)}")

# calculate steps for training
steps_per_epoch = int(np.ceil(train_generator.samples / train_generator.batch_size))
validation_steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))

print(f"Steps per epoch: {steps_per_epoch}")
print(f"Validation steps: {validation_steps}")

"""##Model Architecture (25 pts)

**Model Structure:**

This is a sequential Convolutional Neural Network (CNN) starts with an input layer designed for 32x32 RGB images, followed by three convolutional blocks. Each block consists of a convolutional layer (with 32, 64, and 128 filters respectively), ReLU activation, and max pooling to reduce spatial dimensions. After feature extraction, the output is flattened and passed through a dense layer with 128 units and ReLU activation, followed by a dropout layer to prevent overfitting. The final layer is a single-unit dense layer with sigmoid activation, outputting a probability suitable for binary classification.
***
**Reasoning for training tasks:**

This architecture is well-suited for image classification tasks. CNNs excel at learning spatial features from images, and the layered design allows the model to learn increasingly complex patterns. The final sigmoid layer is ideal for the binary nature of the task (cancer vs. no cancer).

***
**Hyperparameter Tuning**

Number of filters in the convolutional layers were adjusted as hyperparameter tuning. The adjustment of the number of filters in the convolutional layers to (64, 128, 256), from generally larger than the baseline model's filters (32, 64, 128). Increasing the number of filters allows the model to learn a richer set of features, potentially improving its ability to discriminate between cancerous and non-cancerous images.

***
**Different Model Architectures Comparison**

he notebook compares a baseline CNN model with a batch normalization CNN model. The batch normalization model incorporates batch normalization layers after each convolutional and dense layer, which are absent in the baseline model. These layers help stabilize the learning process and allow for potentially higher learning rates. Additionally, the batch normalization model uses more filters in its convolutional layers (64, 128, 256) compared to the baseline model (32, 64, 128), enabling it to learn richer features. The batch normalization model also includes a ReduceLROnPlateau callback for dynamic learning rate adjustment, which was not used in the baseline model. These architectural and training differences aim to improve the batch normalization model's training efficiency and performance.
"""

baseline_model = models.Sequential([
    # 1st convolutional block
    layers.Input(shape=(32, 32, 3)),
    layers.Conv2D(32, (3, 3), padding='same'),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # 2nd convolutional block
    layers.Conv2D(64, (3, 3), padding='same'),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # 3rd convolutional block
    layers.Conv2D(128, (3, 3), padding='same'),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # flatten and dense layers
    layers.Flatten(),
    layers.Dense(128),
    layers.Activation('relu'),
    layers.Dropout(0.2),

    layers.Dense(1, activation='sigmoid', dtype='float32')])

baseline_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.AUC()])

print("Baseline CNN Model Summary:")
baseline_model.summary()

# callbacks
callbacks_list = [
    callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True),
    # save best model
    callbacks.ModelCheckpoint(
        filepath='best_baseline_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1)]

# train baseline model
print("\nTraining Baseline CNN Model...")
baseline_history = baseline_model.fit(
    train_generator,
    epochs=10,
    steps_per_epoch=steps_per_epoch,
    validation_data=validation_generator,
    validation_steps=validation_steps,
    callbacks=callbacks_list,
    verbose=1)

batchnorm_model = models.Sequential([
    # 1st convolutional block with batch normalization
    layers.Input(shape=(32, 32, 3)),
    layers.Conv2D(64, (3, 3), padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # 2nd convolutional block with batch normalization
    layers.Conv2D(128, (3, 3), padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # 3rd convolutional block with batch normalization
    layers.Conv2D(256, (3, 3), padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    # flatten and dense layers
    layers.Flatten(),
    layers.Dense(256),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.5),

    layers.Dense(1, activation='sigmoid', dtype='float32')])

batchnorm_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.AUC()])

print("BatchNorm CNN Model Summary:")
batchnorm_model.summary()

# callbacks for the BatchNorm model
batchnorm_callbacks = [
    callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True),
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=3,
        min_lr=1e-6),
    # save best model
    callbacks.ModelCheckpoint(
        filepath='best_baseline_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1)]
# ModelCheckpoint for BatchNormalization
batchnorm_callbacks[-1] = callbacks.ModelCheckpoint(
    filepath='best_batchnorm_model.keras',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1)

"""##Results and Analysis (35 pts)

The batch normalization model added Batch Normalization layers after each convolutional and dense layer, which is a key structural modification compared to the baseline model. Batch Normalization helps stabilize the learning process by normalizing the inputs to each layer, reducing the dependence of gradients on the scale of parameters or initial values. This can lead to faster convergence and allow for higher learning rates. You also adjusted the number of filters in the convolutional layers to (64, 128, 256), which are generally larger than the baseline model's filters (32, 64, 128). Increasing the number of filters allows the model to learn a richer set of features, potentially improving its ability to discriminate between cancerous and non-cancerous images.

In terms of hyperparameter tuning, while a systematic search wasn't explicitly performed, you incorporated the ReduceLROnPlateau callback in addition to Early Stopping for the batch normalization model. This callback dynamically reduces the learning rate when the validation loss stops improving, which can help the model converge to a better minimum. The results you presented show that the batch normalization model achieved higher training accuracy and AUC compared to the baseline. However, the validation performance improvements are less pronounced, and there seems to be some fluctuation in the batch norm model's validation metrics, potentially indicating some instability or sensitivity to hyperparameters. This suggests that while batch normalization and increased filters can improve training, careful tuning of the learning rate schedule and other hyperparameters might be needed to fully leverage these changes and achieve better generalization on the validation set. Further analysis of the training and validation curves, as well as exploring different hyperparameter combinations (as discussed previously), would provide more insights into why certain modifications worked and how to further enhance the model's performance.

Result Table
-----------
|Model | Accuracy |  AUC | Val_accuracy | Val_AUC
-------|----------|------|--------------|--------
|Baseline | 0.8826|0.9535|0.8490|0.9271
|Batch_norm|0.9469|0.9880|0.8388|0.9155

"""

# train batch normalization model
print("\nTraining BatchNorm CNN Model...")
batchnorm_history = batchnorm_model.fit(
    train_generator,
    epochs=10,
    steps_per_epoch=steps_per_epoch,
    validation_data=validation_generator,
    validation_steps=validation_steps,
    callbacks=batchnorm_callbacks,
    verbose=1)

# Graph train & valication of both models
plt.subplot(1, 2, 1)
plt.plot(baseline_history.history['auc_1'], label='Train Accuracy')
plt.plot(baseline_history.history['val_auc_1'], label='Validation Accuracy')
plt.title('Baseline Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(batchnorm_history.history['auc'], label='Train Loss')
plt.plot(batchnorm_history.history['val_auc'], label='Validation Loss')
plt.title('Baseline Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

def evaluate_and_plot(model, generator, prediction_steps):

    y_pred = model.predict(generator, steps=prediction_steps, verbose=1)
    y_pred_classes = (y_pred > 0.5).astype(int).flatten()

    y_true = generator.classes[:len(y_pred_classes)]# get labels

    cm = confusion_matrix(y_true, y_pred_classes)

    plt.figure(figsize=(3, 2))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Non-tumor', 'Tumor'],
        yticklabels=['Non-tumor', 'Tumor']
    )
    plt.title(f'Confusion Matrix - {model.name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

    fpr, tpr, _ = roc_curve(y_true, y_pred)     # ROC AUC
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(3, 2))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model.name}')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.show()

    print(classification_report(y_true, y_pred_classes))

    return roc_auc

baseline_results = baseline_model.evaluate(validation_generator, verbose=1)
batchnorm_results = batchnorm_model.evaluate(validation_generator, verbose=1)

metric_names = baseline_model.metrics_names
metric_names[1] = 'accuracy'

# Get additional metrics (AUC)
baseline_auc = evaluate_and_plot(baseline_model, validation_generator, validation_steps)
batchnorm_auc = evaluate_and_plot(batchnorm_model, validation_generator, validation_steps)

"""Result

Discussion/Conclusion (15 pts)

Discuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?

Need delete...

Produce Deliverables: High-Quality, Organized Jupyter Notebook Report, GitHub Repository, and screenshot of Kaggle leaderboard (35 points)

These deliverables serve two purposes- grade for this course and your project portfolio that you can show when you apply for jobs.

If you havenâ€™t used GitHub previously, please find a tutorial and get acquainted with it before the project deadline. For the sake of this project, you can use GitHub to showcase your codebase. In the real world, versioning with GitHub is vital for collaboration. Sometimes Jupyter notebooks donâ€™t seem particularly well-suited to versioning with GitHub due to hard-to-read diffs and the like. If you want to use this project as an opportunity to practice versioning with GitHub, consider something like the following: https://www.reviewnb.com
.
"""

#submit

# test data generator
test_datagen = ImageDataGenerator(rescale=1./255)

# test file names DF
test_files = os.listdir(TEST_DIR)
test_df = pd.DataFrame({
    'id': [os.path.splitext(file)[0] for file in test_files],
    'filename': test_files
})

# make test data gen
submission_batch_size = BATCH_SIZE * 2

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    directory=TEST_DIR,
    x_col='filename',
    y_col=None,  # no labels for test data
    target_size=IMG_SIZE,
    batch_size=submission_batch_size,
    class_mode=None,
    shuffle=False
)

# predict
print("\nGenerating predictions for submission...")
predictions = best_model.predict(
    test_generator,
    verbose=1
)
predicted_classes = (predictions > 0.5).astype(int).flatten()

# save
submission_df = pd.DataFrame({
    'id': test_df['id'][:len(predicted_classes)],
    'label': predicted_classes
})

submission_path = 'submission.csv'
submission_df.to_csv(submission_path, index=False)
print(f"Submission saved to {submission_path}")
print(f"Sample of submission file:\n{submission_df.head()}")